数据集
=======================


数据文件存储在 ``spikezoo/data/`` 路径下，组织形式如下：

.. code-block:: console

    spikezoo
    ├── data
    |   ├── base
    |   ├── reds_base
    |   └── u_caltech
    ├── archs
    ...
    └── utils

.. _dataset_prepare:

数据来源
----------------

1. ``BASE`` 数据集 
^^^^^^^^^^^^^

- **介绍:** 从 ``REDS_BASE`` 数据集中选取部分数据构建的小数据集。
- **地址:** 已内置于 Spike-Zoo 仓库当中。

.. code-block:: console

    base
    ├── test
    │   ├── gt
    │   └── spike
    └── train
        ├── gt
        └── spike

2. ``REDS_BASE`` 数据集 
^^^^^^^^^^^^^

- **介绍:** 由 `Spk2ImgNet <https://github.com/Vspacer/Spk2ImgNet>`_ 基于REDS数据集仿真生成的脉冲-清晰图成对数据集。
- **地址:**  训练集: `下载地址 <https://drive.google.com/file/d/1ozR2-fNmU10gA_TCYUfJN-ahV6e_8Ke7/view>`_ | 测试集: `下载地址 <https://drive.google.com/file/d/12q0yJ7V9KtF_y-ZcCn2B-q0zFP8ysdv3/view>`_

.. code-block:: console

    reds_base
    ├── test
    │   ├── gt
    │   └── spike
    └── train
        ├── gt
        └── spike



3. ``UHSR`` 数据集 
^^^^^^^^^^^^^

- **介绍:** 真实拍摄脉数据集 `UHSR <https://github.com/Evin-X/UHSR>`_ ，包括 ``U-CALTECH`` 和 ``U-CIFAR`` 两部分。（使用该数据集旨在探索不同模型在暗光场景下的泛化性）
- **地址:**  `百度网盘地址 <https://pan.baidu.com/share/init?surl=Qcu4OVHakE6VZfIpjS9vlQ&pwd=asin>`_

.. code-block:: console

    u_caltech
    ├── test
    └── train

参数配置
----------------

以 ``BASE`` 数据集为例，配置类定义如下：

.. code-block:: python

    @dataclass
    class BaseDatasetConfig:
        # ------------------ Not Recommended to Change ------------------
        "Dataset name."
        dataset_name: str = "base"
        "Directory specifying location of data."
        root_dir: Union[str, Path] = Path(__file__).parent.parent / Path("data/base")
        "Image width."
        width: int = 400
        "Image height."
        height: int = 250
        "Spike paried with the image or not."
        with_img: bool = True
        "Dataset spike length for the train data."
        spike_length_train: int = -1
        "Dataset spike length for the test data."
        spike_length_test: int = -1
        "Dir name for the spike."
        spike_dir_name: str = "spike"
        "Dir name for the image."
        img_dir_name: str = "gt"

        # ------------------ Config ------------------
        "Dataset split: train/test. Default set as the 'test' for evaluation."
        split: Literal["train", "test"] = "test"
        "Use the data augumentation technique or not."
        use_aug: bool = False
        "Use cache mechanism."
        use_cache: bool = False
        "Crop size."
        crop_size: tuple = (-1, -1)
        "Rate. (-1 denotes variant)"
        rate: float = 0.6

        # post process
        def __post_init__(self):
            self.spike_length = self.spike_length_train if self.split == "train" else self.spike_length_test
            self.root_dir = Path(self.root_dir) if isinstance(self.root_dir, str) else self.root_dir
            # todo try download
            assert self.root_dir.exists(), f"No files found in {self.root_dir} for the specified dataset `{self.dataset_name}`."
            
参数解释如下：

- ``dataset_name`` : 数据集的名称，如 ``"base"``, ``"reds_base"`` 和 ``"uhsr"``。
- ``root_dir`` : 数据集的根路径。
- ``width`` : 输入脉冲的宽度。
- ``height`` : 输入脉冲的高度。
- ``with_img`` : 输入数据是否包含 GT 清晰图，真实数据集一般设置为 False。
- ``spike_length_train`` : 训练集中输入脉冲的长度，在 BASE 数据集中为 41。（如果设置为 -1，则表示对输出的脉冲不做任何裁剪，可能会导致显存占用较高。）
- ``spike_length_test`` : 测试集中输入脉冲的长度，在 BASE 数据集中为 301。
- ``spike_length`` : 表示实例化数据集的脉冲长度，在 ``__post__init`` 中会自动赋值。
- ``spike_dir_name`` : 用于存储脉冲数据文件夹的名字，在 BASE 数据集中为 ``spike``。
- ``img_dir_name`` : 用于存储清晰图数据文件夹的名字，在 BASE 数据集中为 ``gt``。
- ``split`` : 表示该数据集是训练集还是测试集。
- ``use_aug`` : 表示是否使用数据增强技术。
- ``use_cache`` : 表示是否使用数据缓存技术。在数据 I/O 较大且 GPU 利用率较低时开启可以加速训练，但可能会增加 RAM 占用。
- ``crop_size`` : 训练时如果使用数据增强技术，裁剪的尺寸大小，默认值为 (-1, -1) 表示不裁剪。
- ``rate`` : 表示脉冲转化系数，在 REDS_BASE 数据集中默认设置为 0.6。


数据加载类
----------------

.. code-block:: python

    class BaseDataset(Dataset):
        # 初始化数据集实例
        def __init__(self, cfg: BaseDatasetConfig):
        # 获取数据集样本总数
        def __len__(self):
        # 获取指定索引的样本（统一接口返回字典）
        def __getitem__(self, idx: int):
        # 数据路径预处理
        def prepare_data(self):
        # 脉冲文件检索方法
        def get_spike_files(self, path: Path):
        # 脉冲加载逻辑（支持.dat/.npz格式）
        def load_spike(self, idx):
        # 脉冲获取统一接口
        def get_spike(self, idx):
        # 图像文件检索方法
        def get_image_files(self, path: Path):
        # 图像读取接口
        def get_img(self, idx):
        # 数据缓存机制实现
        def cache_data(self):

实例化
----------------

.. code-block:: python

    from spikezoo.datasets import BaseDataset,BaseDatasetConfig
    cfg = BaseDatasetConfig()
    dataset = BaseDataset(cfg)
    batch = dataset[0]
    for key,val in batch.items():
        print(key,val)

输出样本为字典格式，包含以下键值：

- ``spike`` : 脉冲张量（形状 [T,H,W]）

- ``gt_img`` : 清晰图像张量（形状 [3,H,W]）

- ``rate`` : 脉冲转化系数标量

自定义数据集开发
----------------
以下以Spike-Zoo标准仿真生成的数据集为例，说明如何扩展基础数据集类：

**目录结构要求：**

.. code-block:: console

    root
    ├── test
    │   ├── sharp_data
    │   └── spike_data
    └── train
        ├── sharp_data
        └── spike_data

**实现步骤：**

1. 创建配置文件 ``spikezoo/datasets/custom_dataset.py``

.. code-block:: python

    @dataclass
    class CustomDataConfig(BaseDatasetConfig):
        # 数据集名称，和文件名保持一致
        dataset_name: str = "custom"
        # 设定路径
        root_dir: Path = Path(__file__).parent.parent / Path("data/dataset")
        # 宽度为400
        width: int = 400
        # 高度为250
        height: int = 250
        # 包含成对清晰图
        with_img: bool = True
        # 默认长度输入
        spike_length_train: int = -1
        # 默认长度输入
        spike_length_test: int = -1
        # 储存脉冲数据的文件夹名称为 'spike_data'
        spike_dir_name: str = "spike_data"
        # 储存清晰图数据的文件夹名称为 'sharp_data'
        img_dir_name: str = "sharp_data"
        # 设置为1
        rate: float = 1

2. 继承基础数据集类实现数据加载

.. code-block:: python

    class CustomDataset(BaseDataset):
        def __init__(self, cfg: BaseDatasetConfig):
            super(CustomDataset, self).__init__(cfg)

3. 使用自定义数据集

.. code-block:: python

    from spikezoo.datasets import CustomDataConfig, CustomDataset
    cfg = CustomDataConfig()
    train_set = CustomDataset(cfg)