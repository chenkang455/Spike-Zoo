数据集
=======================

Spike-Zoo当前支持 ``Spk2ImgNet`` 论文发布的 ``REDS_BASE`` 和 ``UHSR`` 数据集。其中BASE数据集作为REDS_BASE的子集，
已预置于 ``spikezoo/data/base`` 目录，无需额外下载。

数据集模块采用 **配置类+实现类** 的设计模式，每个数据集包含参数配置类 ``BaseDatasetConfig`` 和数据加载类 ``BaseDataset``。

参数配置
----------------

以BASE数据集为例，目录结构如下：

.. code-block:: console

    base
    ├── test
    │   ├── gt
    │   └── spike
    └── train
        ├── gt
        └── spike

数据集配置类定义如下：

.. code-block:: python

    @dataclass
    class BaseDatasetConfig:
        # ------------------ Not Recommended to Change ------------------
        "Dataset name."
        dataset_name: str = "base"
        "Directory specifying location of data."
        root_dir: Union[str, Path] = Path(__file__).parent.parent / Path("data/base")
        "Image width."
        width: int = 400
        "Image height."
        height: int = 250
        "Spike paried with the image or not."
        with_img: bool = True
        "Dataset spike length for the train data."
        spike_length_train: int = -1
        "Dataset spike length for the test data."
        spike_length_test: int = -1
        "Dir name for the spike."
        spike_dir_name: str = "spike"
        "Dir name for the image."
        img_dir_name: str = "gt"

        # ------------------ Config ------------------
        "Dataset split: train/test. Default set as the 'test' for evaluation."
        split: Literal["train", "test"] = "test"
        "Use the data augumentation technique or not."
        use_aug: bool = False
        "Use cache mechanism."
        use_cache: bool = False
        "Crop size."
        crop_size: tuple = (-1, -1)
        "Rate. (-1 denotes variant)"
        rate: float = 0.6

        # post process
        def __post_init__(self):
            self.spike_length = self.spike_length_train if self.split == "train" else self.spike_length_test
            self.root_dir = Path(self.root_dir) if isinstance(self.root_dir, str) else self.root_dir
            # todo try download
            assert self.root_dir.exists(), f"No files found in {self.root_dir} for the specified dataset `{self.dataset_name}`."
            
参数解释如下：

- ``dataset_name`` : 数据集的名称，如 ``'base'``, ``'reds_base'`` 和 ``'uhsr'``。
- ``root_dir`` : 数据集的根路径。
- ``width`` : 输入脉冲的宽度。
- ``height`` : 输入脉冲的高度。
- ``with_img`` : 输入数据是否包含GT清晰图，真实数据集一般设置为False。
- ``spike_length_train`` : 训练集中输入脉冲的长度，在BASE中是41。（如果设置为-1则表示对输出的脉冲不做任何裁剪，可能会带来高显存问题。）
- ``spike_length_test`` : 测试集中输入脉冲的长度，在BASE中是301。
- ``spike_length``: 表示实例化数据集的脉冲长度，在 ``__post__init`` 中自动赋值。
- ``spike_dir_name`` : 用于存储脉冲数据文件夹的名字，在BASE数据集中是spike。
- ``img_dir_name`` : 用于存储清晰图数据文件夹的名字，在BASE数据集中是gt。
- ``split`` : 表示该数据集是分类，训练集还是测试集。
- ``use_aug`` : 表示是否使用数据增强技术。
- ``use_cache`` : 表示是否使用数据缓存技术，在数据IO较大且GPU利用率较低时开启可以增加训练速度，但是会带来高RAM占用。
- ``crop_size`` : 训练时如果使用数据增强技术，裁剪的尺寸大小，默认(-1，-1)不裁剪。
- ``rate`` : 表示脉冲转化系数，在REDS_BASE数据集中设置默认为0.6。


数据加载类
----------------

.. code-block:: python

    class BaseDataset(Dataset):
        # 初始化数据集实例
        def __init__(self, cfg: BaseDatasetConfig):
        # 获取数据集样本总数
        def __len__(self):
        # 获取指定索引的样本（统一接口返回字典）
        def __getitem__(self, idx: int):
        # 数据路径预处理
        def prepare_data(self):
        # 脉冲文件检索方法
        def get_spike_files(self, path: Path):
        # 脉冲加载逻辑（支持.dat/.npz格式）
        def load_spike(self, idx):
        # 脉冲获取统一接口
        def get_spike(self, idx):
        # 图像文件检索方法
        def get_image_files(self, path: Path):
        # 图像读取接口
        def get_img(self, idx):
        # 数据缓存机制实现
        def cache_data(self):

实例化
----------------

.. code-block:: python

    from spikezoo.datasets import BaseDataset,BaseDatasetConfig
    cfg = BaseDatasetConfig()
    dataset = BaseDataset(cfg)
    batch = dataset[0]
    for key,val in batch.items():
        print(key,val)

输出样本为字典格式，包含以下键值：

- ``spike`` : 脉冲张量（形状 [T,H,W]）

- ``gt_img`` : 清晰图像张量（形状 [3,H,W]）

- ``rate`` : 脉冲转化系数标量

自定义数据集开发
----------------
以下以Spike-Zoo标准仿真生成的数据集为例，说明如何扩展基础数据集类：

**目录结构要求：**

.. code-block:: console

    root
    ├── test
    │   ├── sharp_data
    │   └── spike_data
    └── train
        ├── sharp_data
        └── spike_data

**实现步骤：**

1. 创建配置文件 ``spikezoo/datasets/custom_dataset.py``

.. code-block:: python

    @dataclass
    class CustomDataConfig(BaseDatasetConfig):
        # 数据集名称，和文件名保持一致
        dataset_name: str = "custom"
        # 设定路径
        root_dir: Path = Path(__file__).parent.parent / Path("data/dataset")
        # 宽度为400
        width: int = 400
        # 高度为250
        height: int = 250
        # 包含成对清晰图
        with_img: bool = True
        # 默认长度输入
        spike_length_train: int = -1
        # 默认长度输入
        spike_length_test: int = -1
        # 储存脉冲数据的文件夹名称为 'spike_data'
        spike_dir_name: str = "spike_data"
        # 储存清晰图数据的文件夹名称为 'sharp_data'
        img_dir_name: str = "sharp_data"
        # 设置为1
        rate: float = 1

2. 继承基础数据集类实现数据加载

.. code-block:: python

    class CustomDataset(BaseDataset):
        def __init__(self, cfg: BaseDatasetConfig):
            super(CustomDataset, self).__init__(cfg)

3. 使用自定义数据集

.. code-block:: python

    from spikezoo.datasets import CustomDataConfig, CustomDataset
    cfg = CustomDataConfig()
    train_set = CustomDataset(cfg)